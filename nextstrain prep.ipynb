{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6fd4fd",
   "metadata": {},
   "source": [
    "# module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general needs\n",
    "import os  \n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from google.cloud import storage\n",
    "\n",
    "# biopython\n",
    "from Bio import SeqIO, AlignIO\n",
    "from Bio.SeqRecord import SeqRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd7e3cf",
   "metadata": {},
   "source": [
    "# date input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07940b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current date\n",
    "current_date = str(date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7db14",
   "metadata": {},
   "source": [
    "# creating working directories and subdirectories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define working directory (do NOT change this)\n",
    "home = '/home/diana_ir/Documents/nextstrain' \n",
    "\n",
    "# define GCP working bucket without including the gs:// prefix\n",
    "bucket =  '/home/diana_ir/Documents/nextstrain'\n",
    "\n",
    "# define scripts path\n",
    "scripts = '/home/diana_ir/scripts'\n",
    "\n",
    "# creating main_directory \n",
    "main_dir = os.path.join(home, 'covid_sequencing')\n",
    "if not os.path.exists(main_dir):\n",
    "    os.mkdir(main_dir)\n",
    "\n",
    "# creating subdirectories for main directory   \n",
    "assembly_metrics_dir = os.path.join(main_dir, 'assembly_metrics')\n",
    "if not os.path.exists(assembly_metrics_dir):\n",
    "    os.mkdir(assembly_metrics_dir)\n",
    "\n",
    "concat_metrics_dir = os.path.join(main_dir, 'concat_metrics')\n",
    "if not os.path.exists(concat_metrics_dir):\n",
    "    os.mkdir(concat_metrics_dir)\n",
    "\n",
    "fasta_files_dir = os.path.join(main_dir, 'fasta_files')\n",
    "if not os.path.exists(fasta_files_dir):\n",
    "    os.mkdir(fasta_files_dir)\n",
    "    \n",
    "out_data_dir = os.path.join(main_dir, 'out_data')\n",
    "if not os.path.exists(out_data_dir):\n",
    "    os.mkdir(out_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb81ef2",
   "metadata": {},
   "source": [
    "# exclusion sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37570104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wastewater samples\n",
    "wwt_runs = [\n",
    "    \n",
    "    'COVMIN_0008', 'COVMIN_0013', 'Covseq_WW5', 'COVSEQ_WW6', 'COVMIN_0054', 'COVSEQ_CJ2021_WGS']\n",
    "\n",
    "# bad sequences\n",
    "bad_seq = [\n",
    "    \n",
    "    'COVARC_001', 'COVARC_001rr', 'COVARC_002', 'COVARC_002rex', 'COVARC_002rr', 'COVARC_003', \n",
    "    'COVIDSeq_Test2_NextSeq550', 'COVMIN_Midnight3', 'COVMIN_Midnight4', 'COVMIN_midnight_02',\n",
    "    \n",
    "    'COVMIN_0028', 'COVMIN_0029', 'COVMIN_0055', 'COVMIN_0093', 'COVMIN_0107', 'COVMIN_0108', 'COVMIN_0136', \n",
    "    'COVMIN_0154', 'COVMIN_0156', 'COVMIN_0159', 'COVMIN_0167', 'COVMIN_0167rr','COVMIN_0196', 'COVMIN_0197', \n",
    "    'COVMIN_0198','COVMIN_0202', 'COVMIN_0203', 'COVMIN_0218', 'COVMIN_0222', 'COVMIN_0263', 'COVMIN_0264', \n",
    "    'COVMIN_0294', 'COVMIN_0295', \n",
    "    \n",
    "    'COVMIN_0312', 'COVMIN_0340','COVMIN_0353', 'COVMIN_0363', 'COVMIN_0390', 'COVMIN_0395', 'COVMIN_0402', \n",
    "    'COVMIN_0413', 'COVMIN_0438', 'COVMIN_0445', 'COVMIN_0483', 'COVMIN_0486', 'COVMIN_0487', 'COVMIN_0503', \n",
    "    'COVMIN_0572', 'COVMIN_0609','COVMIN_0625', 'COVMIN_0642', 'COVMIN_0647', 'COVMIN_0648', 'COVMIN_0649',\n",
    "    'COVMIN_0650', 'COVMIN_0653',\n",
    "    \n",
    "    'COVMIN_0741', 'COVMIN_0759', 'COVMIN_0772', 'COVMIN_0813', 'COVMIN_0827', 'COVMIN_0828', 'COVMIN_0829', \n",
    "    'COVMIN_0830', 'COVMIN_0831',\n",
    "    \n",
    "    'COVSEQ_0049', 'COVSEQ_0051', 'COVSEQ_0062', 'COVSEQ_0120', 'COVSEQ_0018', 'COVSEQ_0037','COVSEQ_0047', \n",
    "    'COVSEQ_0048'\n",
    "\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ed7e0",
   "metadata": {},
   "source": [
    "# inclusion sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covmin sequence rereuns\n",
    "covmin_reruns = [ \n",
    "    \n",
    "    'COVMIN_0009', 'COVMIN_0010', 'COVMIN_0011', 'COVMIN_0012', 'COVMIN_0014', 'COVMIN_0015', 'COVMIN_0016', \n",
    "    'COVMIN_0017', 'COVMIN_0018', 'COVMIN_0019', 'COVMIN_0020','COVMIN_0021', 'COVMIN_0022', 'COVMIN_0023', \n",
    "    'COVMIN_0024', 'COVMIN_0025', 'COVMIN_0026', 'COVMIN_0027', 'COVMIN_0028rr','COVMIN_0029rr',\n",
    "    \n",
    "    'COVMIN_0030', 'COVMIN_0031', 'COVMIN_0032', 'COVMIN_0033', 'COVMIN_0034', 'COVMIN_0035', 'COVMIN_0036',  \n",
    "    'COVMIN_0037','COVMIN_0038', 'COVMIN_0039', 'COVMIN_0040', 'COVMIN_0041', 'COVMIN_0042', 'COVMIN_0043', \n",
    "    'COVMIN_0044', 'COVMIN_0045', 'COVMIN_0046', 'COVMIN_0047', 'COVMIN_0048', 'COVMIN_0049',\n",
    "    \n",
    "    'COVMIN_0050', 'COVMIN_0051', 'COVMIN_0052', 'COVMIN_0053', 'COVMIN_0055rr', 'COVMIN_0056', 'COVMIN_0057',\n",
    "    'COVMIN_0058', 'COVMIN_0059', 'COVMIN_0060', 'COVMIN_0061', 'COVMIN_0062', 'COVMIN_0063', 'COVMIN_0064',\n",
    "    'COVMIN_0065', 'COVMIN_0477', 'COVMIN_0489', 'COVMIN_0490', 'COVMIN_0491', 'COVMIN_0492',  'COVMIN_0493', \n",
    "    'COVMIN_0495', 'COVMIN_0496', 'COVMIN_0497', 'COVMIN_0498', 'COVMIN_0499', 'COVMIN_0500'\n",
    "]\n",
    "\n",
    "# archived sample runs\n",
    "covarc_runs = [\n",
    "    \n",
    "    'NEXSEQ_001', 'NEXSEQ_002', 'NEXSEQ_003', 'NEXSEQ_004', 'NEXSEQ_005', 'NEXSEQ_006', 'NEXSEQ_007', \n",
    "    'NEXSEQ_008', 'NEXSEQ_009', 'NEXSEQ_010', 'NEXSEQ_011', 'NEXSEQ_013', 'NEXSEQ_014','NEXSEQ_015', \n",
    "    'NEXSEQ_016', 'NEXSEQ_017'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d0fa1",
   "metadata": {},
   "source": [
    "# get a list of all the sequence runs in the GCP covid_terra bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of seq_runs present in the covid_terra bucket\n",
    "bucket_name = 'covid_terra'\n",
    "bucket_prefix = os.path.join('')\n",
    "\n",
    "client_storage = storage.Client()\n",
    "blobs = client_storage.list_blobs(bucket_name, prefix = bucket_prefix)\n",
    "\n",
    "n = 0\n",
    "seq_runs = []\n",
    "regrexs = [re.compile('COVSEQ'), re.compile('COVMIN'), re.compile('Covseq'), \n",
    "           re.compile('COVARC'), re.compile('NEXSEQ'), re.compile('COVID'), re.compile('NOVASEQ')]\n",
    "\n",
    "for blob in blobs:\n",
    "    seq_run = re.findall('([0-9a-zA-Z_\\-]+)', blob.name)[0]\n",
    "    if (any(regrex.match(seq_run) for regrex in regrexs) and \n",
    "        seq_run not in seq_runs and \n",
    "        seq_run not in wwt_runs and \n",
    "        seq_run not in bad_seq and \n",
    "        not re.search('WW', seq_run)):\n",
    "        \n",
    "        n = n + 1\n",
    "        seq_runs.append(seq_run)\n",
    "        print(n, \"  \", seq_run)\n",
    "print(len(seq_runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59faa1",
   "metadata": {},
   "source": [
    "# pull and concatenate the assembly metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674dc0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for seq_run in seq_runs:\n",
    "    if re.search('COVSEQ', seq_run):\n",
    "        assembly_metrics_terra_bucket = 'covid_terra/%s/terra_outputs/%s_sequence_assembly_metrics.csv' % (seq_run, seq_run)\n",
    "    \n",
    "    elif re.search('COVMIN', seq_run) and seq_run in covmin_reruns:\n",
    "        assembly_metrics_terra_bucket = 'covid_terra/%s/terra_outputs_rr/%s_sequence_assembly_metrics.csv' % (seq_run, seq_run)\n",
    "        \n",
    "    elif re.search('COVMIN', seq_run):\n",
    "        assembly_metrics_terra_bucket = 'covid_terra/%s/terra_outputs/%s_sequence_assembly_metrics.csv' % (seq_run, seq_run)\n",
    "    \n",
    "    elif re.search('COVARC', seq_run):\n",
    "        assembly_metrics_terra_bucket = 'covid_terra/%s/terra_outputs/%s_sequence_assembly_metrics.csv' % (seq_run, seq_run)\n",
    "    \n",
    "    elif re.search('NOVASEQ', seq_run):\n",
    "        assembly_metrics_terra_bucket = 'covid_terra/%s/terra_outputs/%s_sequence_assembly_metrics.csv' % (seq_run, seq_run)\n",
    "    \n",
    "    elif re.search('NEXSEQ', seq_run):\n",
    "        assembly_metrics_terra_bucket = 'covid_terra/%s/terra_outputs/%s_sequence_assembly_metrics.csv' % (seq_run, seq_run)\n",
    "    \n",
    "    \n",
    "  # check to see if that assembly metrics file already exists in the assembly_metrics_dir\n",
    "    assembly_metrics_path = os.path.join(assembly_metrics_dir, '%s_sequence_assembly_metrics.csv' % seq_run)\n",
    "    \n",
    "    if not os.path.exists(assembly_metrics_path):\n",
    "        print('adding %s_sequence_assembly_metrics.csv to assembly_metrics directory' % seq_run)\n",
    "        !gsutil -m cp -r gs://{assembly_metrics_terra_bucket} {assembly_metrics_dir}\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "# concatendate the files into one\n",
    "os.chdir(assembly_metrics_dir)\n",
    "\n",
    "data_frame_list = []\n",
    "for file in glob.glob('*.csv'):\n",
    "    d = pd.read_csv(file, dtype = {'accession_id' : object})\n",
    "    data_frame_list.append(d)\n",
    "\n",
    "df = pd.concat(data_frame_list)\n",
    "\n",
    "# save it\n",
    "outfile = os.path.join(concat_metrics_dir, 'concatenated_assembly_metrics_%s.csv' % current_date)\n",
    "df.to_csv(outfile, index=False)\n",
    "\n",
    "#check the size\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ce8b0",
   "metadata": {},
   "source": [
    "# removing specific \"bad samples\" from the rerun tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6329620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the rerun samples spreadsheet \n",
    "# https://docs.google.com/spreadsheets/d/1PtaHHa8iuqM6fNGTSkHiinxc5poQnVt1Ti67yLSVQ6c/edit#gid=0\n",
    "path = '/home/diana_ir/Documents/nextstrain/bad_noodles/re-run_samples.tsv'\n",
    "re_runs = pd.read_csv(path, sep = '\\t', dtype = {'accession_id' : object})\n",
    "print(re_runs.shape)\n",
    "\n",
    "# path to the concatenated dataset (CHANGE)\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/concat_metrics/concatenated_metrics_bad_samples_removed_2022-07-14.csv'\n",
    "df = pd.read_csv(path, dtype = {'accession_id' : object})\n",
    "print(df.shape)\n",
    "\n",
    "crit = ~df.accession_id.isin(re_runs)\n",
    "df = df[crit]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# remove re-run samples\n",
    "print('... removing re-run samples')\n",
    "print('...... num rows before: %d' % df.shape[0])\n",
    "\n",
    "\n",
    "# create a dictionary of accession id and runs to drop...\n",
    "re_runs_dict = {}\n",
    "for row in range(re_runs.shape[0]):\n",
    "    accession_id = re_runs.accession_id[row]\n",
    "    first_run = re_runs.first_run[row]\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    if re.search('/', str(first_run)):\n",
    "        first_runs = first_run.split('/')\n",
    "        first_runs_list = []\n",
    "        for first_run in first_runs:\n",
    "            first_runs_list.append(first_run)\n",
    "        re_runs_dict[accession_id] = first_runs_list\n",
    "    elif re.search('\\(', str(first_run)):\n",
    "        re_runs_dict[accession_id] = first_run.split('(')[0]\n",
    "    else:\n",
    "        re_runs_dict[accession_id] = first_run\n",
    "\n",
    "    print(accession_id, ': ', re_runs_dict[accession_id])\n",
    "        \n",
    "# identify indexes (rows) with re-run samples\n",
    "indexes_to_drop = []\n",
    "for row in range(df.shape[0]):\n",
    "    accession_id = df.accession_id[row]\n",
    "    if accession_id in re_runs_dict.keys():\n",
    "        if df.seq_run[row] in str(re_runs_dict[accession_id]):\n",
    "            indexes_to_drop.append(row)\n",
    "\n",
    "#drop those rows\n",
    "df = df[~df.index.isin(indexes_to_drop)]\n",
    "print('...... num rows after: %d' % df.shape[0])\n",
    "\n",
    "# # save it\n",
    "outfile = os.path.join(concat_metrics_dir, 'concatenated_metrics_bad_samples_removed_%s.csv' % current_date)\n",
    "df.to_csv(outfile, index=False)\n",
    "\n",
    "#check the size\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adaf4a2",
   "metadata": {},
   "source": [
    "# removing weird characters in the fasta file (Molly had mentioned -1 or -2 in the accession ids) which might affect merging, if files are already in the directory downloaded previously, no need to rerun this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the \"-1\" samples...\n",
    "# read concatenated metrics file...\n",
    "outfile = os.path.join(concat_metrics_dir, 'concatenated_metrics_bad_samples_removed_%s.csv' % current_date)\n",
    "metrics = pd.read_csv(outfile, dtype = {'accession_id' : object})\n",
    "\n",
    "\n",
    "for seq_run in seq_runs:\n",
    "    if re.search('NEXSEQ', seq_run) or seq_run in covarc_runs:\n",
    "        seq_run_subdir = os.path.join(fasta_files_dir, seq_run)\n",
    "        os.chdir(seq_run_subdir)\n",
    "\n",
    "        for file in glob.glob('*'):\n",
    "            if re.search('\\d{10}-\\d_consensus_renamed.fa', file):\n",
    "\n",
    "                print('correcting %s in %s' % (file, seq_run))\n",
    "                accession_id = re.findall('(\\d{10})-\\d_consensus_renamed.fa', file)[0]\n",
    "\n",
    "                # check to make sure that the -1 and non -1 accession were not run in the same seq_run\n",
    "                # otherwise the -1 will overwrite the non -1\n",
    "                # will keep the onew ith the higher percent coverage....\n",
    "\n",
    "                crit = metrics.seq_run == seq_run\n",
    "                temp = metrics[crit]\n",
    "                og_accessions = temp.OG_accession_id.unique().tolist()\n",
    "\n",
    "                if accession_id in og_accessions:\n",
    "                    print('\"-1\" and non \"-1\" sample on same sequencing run... check for higher percent coverage')\n",
    "\n",
    "                    crit = temp.accession_id == accession_id\n",
    "                    temp = temp[crit]\n",
    "\n",
    "                    temp = temp.sort_values(by = 'percent_non_ambigous_bases', ascending = False)\n",
    "                    temp = temp.reset_index(drop = True)\n",
    "\n",
    "                    higher_cvg_sample = temp.loc[0, 'OG_accession_id']\n",
    "\n",
    "                    print('sample with higher coverage == %s' % higher_cvg_sample)\n",
    "\n",
    "                    if re.search('\\d{10}-\\d', higher_cvg_sample):\n",
    "                        print('-1 sample will overwrite non -1 sample')\n",
    "\n",
    "\n",
    "                        # rename the file\n",
    "                        new_file_name = os.path.join(seq_run_subdir, '%s_consensus_renamed.fa' % accession_id)\n",
    "\n",
    "                        # correct fasta header...\n",
    "                        record = SeqIO.read(file, 'fasta')\n",
    "                        new_id = 'CO-CDPHE-%s' % accession_id\n",
    "                        new_record = SeqRecord(record.seq, id = new_id, name = new_id, description = '' )\n",
    "\n",
    "                        with open(new_file_name, 'w') as handle:\n",
    "                            SeqIO.write(new_record, handle, 'fasta')\n",
    "                else:\n",
    "                    # rename the file\n",
    "                    new_file_name = os.path.join(seq_run_subdir, '%s_consensus_renamed.fa' % accession_id)\n",
    "\n",
    "                    # correct fasta header...\n",
    "                    record = SeqIO.read(file, 'fasta')\n",
    "                    new_id = 'CO-CDPHE-%s' % accession_id\n",
    "                    new_record = SeqRecord(record.seq, id = new_id, name = new_id, description = '' )\n",
    "\n",
    "                    with open(new_file_name, 'w') as handle:\n",
    "                        SeqIO.write(new_record, handle, 'fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022aada4",
   "metadata": {},
   "source": [
    "# Dropping columns and removing samples and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2284ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file (CHANGED EACH TIME)\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/concat_metrics/concatenated_assembly_metrics_2022-07-14.csv'\n",
    "df = pd.read_csv(path, dtype = {'accession_id' : object})\n",
    "print('original', df.shape)\n",
    "\n",
    "# drop columns from the concat metrics bad samples removed file\n",
    "drop_col = [\n",
    "    'plate_name', 'plate_sample_well', 'primer_set', 'omicron_spike_mutations', 'delta_plus_spike_mutations', \n",
    "    'total_nucleotide_mutations', 'total_AA_substitutions', 'total_AA_deletions', 'mean_depth', \n",
    "    'number_aligned_bases', 'number_non_ambigous_bases', 'number_seqs_in_fasta', 'total_nucleotide_deletions',\n",
    "    'total_nucleotide_insertions', 'num_reads', 'mean_base_quality', 'mean_map_quality', \n",
    "    'number_N_bases', 'tech_platform', 'read_type', 'analysis_date', 'samtools_mean_depth', \n",
    "    'samtools_percent_cvg', 'samtools_mean_base_quality', 'samtools_mean_map_quality', 'assembler_version', \n",
    "    'mean_map_quality.1', 'mean_depth.1', 'mean_base_quality.1', 'num_reads.1'\n",
    "        ]\n",
    "\n",
    "## if you run the previous step, then these variables need to be on the drop_col\n",
    "# 'fasta_header' 'OG_accession_id' 'patient_state' 'patient_county'\\n 'collection_date' \n",
    "# 'customer_name' 'first_name' 'last_name' 'dob'\\n 'patient_zip' 'receive_date' \n",
    "# 'first_name' 'last_name' 'dob'\n",
    "\n",
    "df = df.drop(columns = drop_col)\n",
    "print('removing columns', df.shape)\n",
    "\n",
    "# removing specific \"keywords\" in the spreadsheet \n",
    "df = df[df[\"accession_id\"].str.contains(\"POS\")==False]\n",
    "df = df[df[\"accession_id\"].str.contains(\"NTC\")==False]\n",
    "df = df[df[\"accession_id\"].str.contains(\"ntc\")==False]\n",
    "df = df[df[\"accession_id\"].str.contains(\"NC\")==False]\n",
    "df = df[df[\"accession_id\"].str.contains(\"DiaplexPositive\")==False]\n",
    "df = df[df[\"accession_id\"].str.contains(\"ExtractionPositive\")==False]\n",
    "df = df[df[\"accession_id\"].str.contains(\"Control\")==False]\n",
    "df = df[df[\"accession_id\"].str.contains(\"Contaminated\")==False]\n",
    "df = df[df[\"accession_id\"].str.contains(\"PC\")==False]\n",
    "df = df[df[\"accession_id\"].str.contains(\"Blank\")==False]\n",
    "print('removing specific keywords in the spreadsheet', df.shape)\n",
    "\n",
    "# renaming specific samples in the spreadsheet\n",
    "df['accession_id'] = df['accession_id'].replace({'2005230549-1': '2005230549'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2005230619-1': '2005230619'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2102326037-1': '2102326037'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2002280169-1': '2002280169'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2003090165-1': '2003090165'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2005190431-1': '2005190431'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2005190442-1': '2005190442'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2005190513-1': '2005190513'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2005190587-1': '2005190587'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2005230137-1': '2005230137'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2007090800-1': '2007090800'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2007090806-1': '2007090806'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2007090831-1': '2007090831'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2007090850-1': '2007090850'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2004150183-1': '2004150183'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2004180099-1': '2004180099'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2004200476-1': '2004200476'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2004250387-1': '2004250387'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2102455039-1': '2102455039'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2100820125-1': '2100820125'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2005160401-1': '2005160401'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2005240111-1': '2005240111'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2102326037-2': '2102326037'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2102455039-2': '2102455039'})\n",
    "df['accession_id'] = df['accession_id'].replace({'2100820125-2': '2100820125'})\n",
    "df['accession_id'] = df['accession_id'].replace({'': ''})\n",
    "df['accession_id'] = df['accession_id'].replace({'': ''})\n",
    "df['accession_id'] = df['accession_id'].replace({'': ''})\n",
    "df['accession_id'] = df['accession_id'].replace({'': ''})\n",
    "df['accession_id'] = df['accession_id'].replace({'': ''})\n",
    "print('renaming samples', df.shape)\n",
    "\n",
    "\n",
    "# filter out the bad runs and waste water samples...\n",
    "crit = ~df.seq_run.isin(bad_seq)\n",
    "crit2 = ~df.seq_run.isin(wwt_runs)\n",
    "crit3 = df.accession_id.str.contains('^2')\n",
    "ww = df[crit & crit2 & crit3]\n",
    "print('removing bad runs, WWT runs', ww.shape)\n",
    "\n",
    "# filter samples to >90% coverage\n",
    "crit = ww.percent_non_ambigous_bases != 'sample failed assembly'\n",
    "ww = ww[crit]\n",
    "ww.percent_non_ambigous_bases = ww.percent_non_ambigous_bases.astype('float')\n",
    "print(ww.dtypes)\n",
    "print(ww.shape)\n",
    "crit = ww.percent_non_ambigous_bases >= 90\n",
    "flit_df = ww[crit]\n",
    "print('filtering based on 90 percent coverage', flit_df.shape)\n",
    "\n",
    "# drop duplicates\n",
    "drop = flit_df.sort_values(by ='percent_non_ambigous_bases', ascending = False)\n",
    "drop = flit_df.drop_duplicates(subset = 'accession_id', keep = 'first')\n",
    "print('dropping duplicates', drop.shape)\n",
    "\n",
    "\n",
    "# save filtered metrics\n",
    "outfile = os.path.join(out_data_dir, 'filtered_assembly_metrics_%s.csv' % current_date)\n",
    "drop.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113d4dc",
   "metadata": {},
   "source": [
    "# download fasta files to directory (if the files don't download, delete the folder and rerun this command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632f102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we are first going to pull all the fasta file directories from the bucket\n",
    "for seq_run in seq_runs:\n",
    "    # create seq_run subdir in the fasta's directory\n",
    "    seq_run_subdir = os.path.join(fasta_files_dir, seq_run)\n",
    "    \n",
    "    if not os.path.exists(seq_run_subdir):\n",
    "        print(seq_run)\n",
    "        os.mkdir(seq_run_subdir)\n",
    "        \n",
    "        if re.search('COVSEQ', seq_run):\n",
    "            bucket_path = 'covid_terra/%s/terra_outputs/assemblies/*consensus_renamed.fa' % (seq_run)\n",
    "        elif re.search('COVMIN', seq_run) and seq_run in covmin_reruns:\n",
    "            bucket_path = 'covid_terra/%s/terra_outputs_rr/assemblies/*consensus_renamed.fa' % (seq_run )\n",
    "        elif re.search('COVMIN', seq_run):\n",
    "            bucket_path = 'covid_terra/%s/terra_outputs/assemblies/*consensus_renamed.fa' % (seq_run )\n",
    "        elif re.search('COVARC', seq_run):\n",
    "            bucket_path = 'covid_terra/%s/terra_outputs/assemblies/*consensus_renamed.fa' % (seq_run )\n",
    "        elif re.search('NOVASEQ', seq_run):\n",
    "            bucket_path = 'covid_terra/%s/terra_outputs/assemblies/*consensus_renamed.fa' % (seq_run )         \n",
    "        elif re.search('NEXSEQ', seq_run):\n",
    "            bucket_path = 'covid_terra/%s/terra_outputs/assemblies/*consensus_renamed.fa' % (seq_run )\n",
    "        \n",
    "        # download the fasta file to the fasta files tmp directory\n",
    "        !gsutil -m cp -r gs://{bucket_path} {seq_run_subdir}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d14263",
   "metadata": {},
   "source": [
    "# filter to 90% cov samples and pull fasta files in together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58edb181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathway to the filtered assembly metrics file (CHANGE)\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/filtered_assembly_metrics_2022-07-14.csv'\n",
    "metadata = pd.read_csv(path, dtype = {'accession_id' : object})\n",
    "print (metadata.shape)\n",
    "\n",
    "# using the fitlered assembly metrics we want to pull out the fasta files for those sequences\n",
    "out_fasta = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/all_sequence_data.fasta'\n",
    "if os.path.isfile(out_fasta):\n",
    "    os.remove(out_fasta)\n",
    "\n",
    "for row in range(metadata.shape[0]):\n",
    "    accession_id = metadata.accession_id[row]\n",
    "    seq_run = metadata.seq_run[row]\n",
    "    \n",
    "    fasta_file_path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/fasta_files/%s/%s_consensus_renamed.fa' % (seq_run, accession_id)\n",
    "    \n",
    "    print('adding %s_consensus_renamed.fa from %s to concatenated fasta file' % (accession_id, seq_run))\n",
    "    record = SeqIO.read(fasta_file_path, 'fasta')\n",
    "    with open(out_fasta, 'a') as out_handle:\n",
    "        SeqIO.write(record, out_handle, 'fasta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13dbb30",
   "metadata": {},
   "source": [
    "# removing dups from multi-fasta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2266a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils.CheckSum import seguid\n",
    "\n",
    "path = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/all_sequence_data_dups.fasta\"\n",
    "out = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/all_sequence_data.fasta\"\n",
    "\n",
    "def remove_dup_seqs(records):\n",
    "    \"\"\"\"SeqRecord iterator to removing duplicate sequences.\"\"\"\n",
    "    checksums = set()\n",
    "    for record in records:\n",
    "        checksum = seguid(record.seq)\n",
    "        if checksum in checksums:\n",
    "            print (\"Ignoring %s\" % record.id)\n",
    "            continue\n",
    "        checksums.add(checksum)\n",
    "        yield record\n",
    "\n",
    "records = remove_dup_seqs(SeqIO.parse(path, \"fasta\"))\n",
    "count = SeqIO.write(records, out, \"fasta\")\n",
    "print (\"Saved %i records\" % count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1607f5",
   "metadata": {},
   "source": [
    "# clade dataset concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13fdee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download files from GCP\n",
    "download = '/home/diana_ir/Downloads/'\n",
    "clade_01 = 'diana_sandbox/Nextstrain/nextclade_out/group_1_nextclade.csv'\n",
    "clade_02 = 'diana_sandbox/Nextstrain/nextclade_out/group_2_nextclade.csv'\n",
    "clade_03 = 'diana_sandbox/Nextstrain/nextclade_out/group_3_nextclade.csv'\n",
    "clade_04 = 'diana_sandbox/Nextstrain/nextclade_out/group_4_nextclade.csv'\n",
    "clade_05 = 'diana_sandbox/Nextstrain/nextclade_out/group_5_nextclade.csv'\n",
    "clade_06 = 'diana_sandbox/Nextstrain/nextclade_out/group_6_nextclade.csv'\n",
    "\n",
    "!gsutil -m cp -r gs://{clade_01} {download}\n",
    "!gsutil -m cp -r gs://{clade_02} {download}\n",
    "!gsutil -m cp -r gs://{clade_03} {download}\n",
    "!gsutil -m cp -r gs://{clade_04} {download}\n",
    "!gsutil -m cp -r gs://{clade_05} {download}\n",
    "!gsutil -m cp -r gs://{clade_06} {download}\n",
    "\n",
    "\n",
    "pangolin_01 = 'diana_sandbox/Nextstrain/pangolin_out/group_1_pangolin_lineage_report.csv'\n",
    "pangolin_02 = 'diana_sandbox/Nextstrain/pangolin_out/group_2_pangolin_lineage_report.csv'\n",
    "pangolin_03 = 'diana_sandbox/Nextstrain/pangolin_out/group_3_pangolin_lineage_report.csv'\n",
    "pangolin_04 = 'diana_sandbox/Nextstrain/pangolin_out/group_4_pangolin_lineage_report.csv'\n",
    "pangolin_05 = 'diana_sandbox/Nextstrain/pangolin_out/group_5_pangolin_lineage_report.csv'\n",
    "pangolin_06 = 'diana_sandbox/Nextstrain/pangolin_out/group_6_pangolin_lineage_report.csv'\n",
    "\n",
    "\n",
    "!gsutil -m cp -r gs://{pangolin_01} {download}\n",
    "!gsutil -m cp -r gs://{pangolin_02} {download}\n",
    "!gsutil -m cp -r gs://{pangolin_03} {download}\n",
    "!gsutil -m cp -r gs://{pangolin_04} {download}\n",
    "!gsutil -m cp -r gs://{pangolin_05} {download}\n",
    "!gsutil -m cp -r gs://{pangolin_06} {download}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce397a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathway to the nextclade files\n",
    "path1 = \"/home/diana_ir/Downloads/group_1_nextclade.csv\"\n",
    "c1 = pd.read_csv(path1,sep ='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "path2 = \"/home/diana_ir/Downloads/group_2_nextclade.csv\"\n",
    "c2 = pd.read_csv(path2,sep ='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "path3 = \"/home/diana_ir/Downloads/group_3_nextclade.csv\"\n",
    "c3 = pd.read_csv(path3,sep ='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "path4 = \"/home/diana_ir/Downloads/group_4_nextclade.csv\"\n",
    "c4 = pd.read_csv(path4,sep ='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "path5 = \"/home/diana_ir/Downloads/group_5_nextclade.csv\"\n",
    "c5 = pd.read_csv(path5,sep ='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "path6 = \"/home/diana_ir/Downloads/group_6_nextclade.csv\"\n",
    "c6 = pd.read_csv(path6,sep ='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "clade = pd.concat([c1, c2, c3, c4, c5, c6])\n",
    "\n",
    "# save filtered metrics\n",
    "outfile = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/clade_%s.csv\" % (current_date)\n",
    "clade.to_csv(outfile,  index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de6817",
   "metadata": {},
   "source": [
    "# pangolin dataset concatentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathway to the pangolin files\n",
    "path1 = \"/home/diana_ir/Downloads/group_1_pangolin_lineage_report.csv\"\n",
    "p1 = pd.read_csv(path1, sep ='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "path2 = \"/home/diana_ir/Downloads/group_2_pangolin_lineage_report.csv\"\n",
    "p2 = pd.read_csv(path2, sep ='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "path3 = \"/home/diana_ir/Downloads/group_3_pangolin_lineage_report.csv\"\n",
    "p3 = pd.read_csv(path3, sep ='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "path4 = \"/home/diana_ir/Downloads/group_4_pangolin_lineage_report.csv\"\n",
    "p4 = pd.read_csv(path4, sep ='\\t', dtype = {'accession_id' : object})\n",
    "                                            \n",
    "path5 = \"/home/diana_ir/Downloads/group_5_pangolin_lineage_report.csv\"\n",
    "p5 = pd.read_csv(path5, sep ='\\t', dtype = {'accession_id' : object})\n",
    "                                            \n",
    "path6 = \"/home/diana_ir/Downloads/group_6_pangolin_lineage_report.csv\"\n",
    "p6 = pd.read_csv(path6, sep ='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "pangolin = pd.concat([p1, p2, p3, p4, p5, p6])\n",
    "\n",
    "# save filtered metrics\n",
    "outfile = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/pangolin_%s.csv\" % (current_date)\n",
    "pangolin.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e1172",
   "metadata": {},
   "source": [
    "# pangolin and clade dataset merge with cdphe metadata strain dataset and adding the \"required columns - strain, region, country, division, location\" to the metadata, remove duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e700fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "# pangolin\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/pangolin_%s.csv' % (current_date)\n",
    "pangolin = pd.read_csv(path, dtype = {'taxon' : object})\n",
    "pangolin = pangolin.rename(columns = {'lineage' : 'pangolin'})\n",
    "pangolin = pangolin.set_index('taxon')\n",
    "\n",
    "# nextclade\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/clade_%s.csv' % (current_date)\n",
    "nextclade = pd.read_csv(path, dtype = {'seqName' : object})\n",
    "nextclade = nextclade.set_index('seqName')\n",
    "\n",
    "# metadata\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/filtered_assembly_metrics_%s.csv' % (current_date)\n",
    "df = pd.read_csv(path, dtype = {'accession_id' : object})\n",
    "\n",
    "# renaming the accession_id to \"strain\"\n",
    "def create_strain_name(accession_id):\n",
    "    return 'CO-CDPHE-%s' % accession_id\n",
    "\n",
    "strain = df.apply(lambda x:create_strain_name(x.accession_id), axis = 1)\n",
    "df.insert(loc = 0, value = strain, column = 'strain')\n",
    "\n",
    "# adding in the necessary columns\n",
    "df['region'] = 'North America'\n",
    "df['country'] = 'USA'\n",
    "df['division'] = 'Colorado'\n",
    "df['location'] = ''\n",
    "\n",
    "df = df.set_index('strain', drop=False)\n",
    "print(df.shape)\n",
    "\n",
    "# join all pangolin and nexclade\n",
    "j = df.join(pangolin.pangolin, how = 'left')\n",
    "k = j.join(nextclade.clade, how = 'left')\n",
    "k\n",
    "\n",
    "# drop duplicates\n",
    "k = k.drop_duplicates(subset = 'strain', keep = 'first')\n",
    "print(k.shape)\n",
    "\n",
    "# save output file\n",
    "outfile = os.path.join(out_data_dir, 'cdphe_metadata_%s.tsv' % current_date)\n",
    "k.to_csv(outfile, index = False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a377238",
   "metadata": {},
   "source": [
    "# merge horizon data with seq + pangolin + clade dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b9f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/cdphe_metadata_%s.tsv\" % (current_date)\n",
    "metadata = pd.read_csv(metadata, )\n",
    "varseq = \"/home/diana_ir/Desktop/summary_results/cv_var_seq_bioinformatics 2022-04-14.csv\"\n",
    "bicovid = \"/home/diana_ir/Desktop/summary_results/bi_covid_bioinformatics 2021-09-22.csv\"\n",
    "output = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8004b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge horizon with internal dataset\n",
    "!{scripts}/horizon_internal_2.py -i {metadata} --varseq {varseq} --pui {bicovid} -o {output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b72ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge horizon with external dataset\n",
    "!{scripts}/horizon_external_2.py -i {metadata} --varseq {varseq} --pui {bicovid} -o {output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f679d",
   "metadata": {},
   "source": [
    "# filtering concat files to (3 mo prior) and (6 mo prior) date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************internal build files********************\n",
    "\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-internal.tsv'\n",
    "internal = pd.read_csv(path, sep='\\t', dtype = {'accession_id' : object})\n",
    "print(\"original\", internal.shape)\n",
    "\n",
    "# drop columns\n",
    "drop_col = ['first_name', 'last_name', 'dob']\n",
    "internal = internal.drop(columns=drop_col)\n",
    "\n",
    "#  3 mo prior from \"3 mo prior from current date\"\n",
    "internal['date2'] = pd.to_datetime(internal['date'])  \n",
    "mask = (internal['date2'] <= \"2022-02-01\")\n",
    "before = internal.loc[mask]\n",
    "\n",
    "# sample 20000 random samples \n",
    "before = before.sample(n = 20000)\n",
    "\n",
    "# 3 mo prior from current date\n",
    "mask = (internal['date2'] >= \"2022-02-01\")\n",
    "three_in = internal.loc[mask]\n",
    "\n",
    "# merge 3 mo with 20000 random samples \n",
    "three_in_merged = pd.concat([three_in, before])\n",
    "three_in_merged = three_in_merged.reset_index()\n",
    "print(\"merged\", three_in_merged.shape)\n",
    "\n",
    "\n",
    "# remove duplicates\n",
    "three_in_merged = three_in_merged.drop_duplicates(subset = 'strain', keep = 'first')\n",
    "print(\"duplications\", three_in_merged.shape)\n",
    "\n",
    "# drop \"TMI\" columns\n",
    "drop_col = [\"first_name\", \"last_name\", \"dob\"]\n",
    "\n",
    "\n",
    "\n",
    "# **************internal build final files********************\n",
    "\n",
    "outfile = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-internal-before.tsv\"\n",
    "before.to_csv(outfile, sep='\\t', index = False)\n",
    "\n",
    "outfile = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-internal-3m.tsv\"\n",
    "three_in.to_csv(outfile, sep='\\t', index = False)\n",
    "\n",
    "outfile = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-internal-3m-merged.tsv\"\n",
    "three_in_merged.to_csv(outfile, sep='\\t', index = False)\n",
    "\n",
    "\n",
    "# **************external build files********************\n",
    "\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-external.tsv'\n",
    "external = pd.read_csv(path, sep='\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "drop_col = ['first_name', 'last_name', 'dob']\n",
    "external = external.drop(columns=drop_col)\n",
    "\n",
    "#  3 mo prior from \"3 mo prior from current date\"\n",
    "external['date2'] = pd.to_datetime(external['date'])  \n",
    "mask = (external['date2'] <= \"2022-02-01\")\n",
    "prev = external.loc[mask]\n",
    "\n",
    "# sample 20000 random samples \n",
    "prev = prev.sample(n = 20000)\n",
    "\n",
    "\n",
    "# 3 mo\n",
    "mask = (external['date2'] >= \"2022-02-01\")\n",
    "three_ex = external.loc[mask]\n",
    "\n",
    "# merge 3 mo with 20000 random samples \n",
    "three_ex_merged = pd.concat([three_ex, before])\n",
    "three_ex_merged = three_ex_merged.reset_index()\n",
    "\n",
    "# remove duplicates\n",
    "three_ex_merged = three_ex_merged.drop_duplicates(subset = 'strain', keep = 'first')\n",
    "print(three_ex_merged.shape)\n",
    "\n",
    "\n",
    "# **************external build final files********************\n",
    "\n",
    "outfile = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-external-prev.tsv\"\n",
    "prev.to_csv(outfile, sep='\\t', index = False)\n",
    "\n",
    "outfile = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-external-3m.tsv\"\n",
    "three_ex.to_csv(outfile, sep='\\t', index = False)\n",
    "\n",
    "outfile = \"/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-external-3m-merged.tsv\"\n",
    "three_ex_merged.to_csv(outfile, sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6acb0",
   "metadata": {},
   "source": [
    "# filter cov samples to internal 3 months and pull fasta files in together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling metadata-internal-3m-merged fasta files together\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-internal-3m-merged.tsv'\n",
    "metadata = pd.read_csv(path, sep='\\t', dtype = {'accession_id' : object})\n",
    "print (metadata.shape)\n",
    "\n",
    "# filter to samples with >90% coverage \n",
    "crit = metadata.percent_non_ambigous_bases >= 90\n",
    "metadata = metadata[crit]\n",
    "print(metadata.shape)\n",
    "metadata = metadata.reset_index(drop = True)\n",
    "\n",
    "# using the fitlered assembly metrics we want to pull out the internal fasta files for those sequences\n",
    "out_fasta = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-internal-3m-merged_sequence_data.fasta'\n",
    "if os.path.isfile(out_fasta):\n",
    "    os.remove(out_fasta)\n",
    "        \n",
    "for row in range(metadata.shape[0]):\n",
    "    accession_id = metadata.accession_id[row]\n",
    "    seq_run = metadata.seq_run[row]\n",
    "    \n",
    "    fasta_file_path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/fasta_files/%s/%s_consensus_renamed.fa' % (seq_run, accession_id)\n",
    "    \n",
    "    print('adding %s_consensus_renamed.fa from %s to concatenated fasta file' % (accession_id, seq_run))\n",
    "    record = SeqIO.read(fasta_file_path, 'fasta')\n",
    "    with open(out_fasta, 'a') as out_handle:\n",
    "        SeqIO.write(record, out_handle, 'fasta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc2529",
   "metadata": {},
   "source": [
    "# filter cov samples to external 3 months and pull fasta files in together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ff41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling metadata-external-3m-merged fasta files together\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-external-3m-merged.tsv'\n",
    "metadata = pd.read_csv(path, sep='\\t', dtype = {'accession_id' : object})\n",
    "print (metadata.shape)\n",
    "\n",
    "\n",
    "# filter to samples with >90% coverage \n",
    "crit = metadata.percent_non_ambigous_bases >= 90\n",
    "metadata = metadata[crit]\n",
    "print(metadata.shape)\n",
    "metadata = metadata.reset_index(drop = True)\n",
    "\n",
    "# using the fitlered assembly metrics we want to pull out the external fasta files for those sequences\n",
    "out_fasta = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-external-3m-merged_sequence_data.fasta'\n",
    "if os.path.isfile(out_fasta):\n",
    "    os.remove(out_fasta)\n",
    "\n",
    "for row in range(metadata.shape[0]):\n",
    "    accession_id = metadata.accession_id[row]\n",
    "    seq_run = metadata.seq_run[row]\n",
    "    \n",
    "    fasta_file_path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/fasta_files/%s/%s_consensus_renamed.fa' % (seq_run, accession_id)\n",
    "    \n",
    "    print('adding %s_consensus_renamed.fa from %s to concatenated fasta file' % (accession_id, seq_run))\n",
    "    record = SeqIO.read(fasta_file_path, 'fasta')\n",
    "    with open(out_fasta, 'a') as out_handle:\n",
    "        SeqIO.write(record, out_handle, 'fasta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f664a",
   "metadata": {},
   "source": [
    "# filter samples to only delta samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e2a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathway to original metadata internal file\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-internal.tsv'\n",
    "delta = pd.read_csv(path,  sep=\"\\t\", dtype = {'accession_id' : object})\n",
    "print(\"original\", delta.shape)\n",
    "\n",
    "# filtering date 6 months prior to current date, NEXTSTRAIN can't handle >50,000 samples in a build\n",
    "delta['date2'] = pd.to_datetime(delta['date'])  \n",
    "mask = (delta['date2'] >= \"2021-09-01\")\n",
    "filtered = delta.loc[mask]\n",
    "print(\"filtered date\", filtered.shape)\n",
    "\n",
    "# filtering down to B.1.617.2's\n",
    "a01 = filtered.pangolin == ('B.1.617.2')\n",
    "a02 = filtered[\"pangolin\"].str.contains(\"AY\")==True\n",
    "df_filtered = filtered[ a01 | a02 ]\n",
    "print(\"delta filtered\", df_filtered.shape)\n",
    "\n",
    "# save output file as...\n",
    "outfile = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/delta_metadata-internal.tsv'\n",
    "df_filtered.to_csv(outfile, index = False, sep = '\\t')\n",
    "print (df_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc527116",
   "metadata": {},
   "source": [
    "# filter cov samples to internal delta variant only and pull fasta files in together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ffc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathway to delta metadata file\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/delta_metadata-internal.tsv'\n",
    "metadata = pd.read_csv(path, sep = '\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "# remove duplicates\n",
    "metadata = metadata.drop_duplicates(subset = 'strain', keep = 'first')\n",
    "print(metadata.shape)\n",
    "\n",
    "# filter to samples with >90% coverage \n",
    "crit = metadata.percent_non_ambigous_bases >= 90\n",
    "metadata = metadata[crit]\n",
    "print(metadata.shape)\n",
    "metadata = metadata.reset_index(drop = True)\n",
    "\n",
    "# using the fitlered assembly metrics we want to pull out the fasta files for those sequences\n",
    "out_fasta = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/delta_sequence_data.fasta'\n",
    "if os.path.isfile(out_fasta):\n",
    "    os.remove(out_fasta)\n",
    "\n",
    "for row in range(metadata.shape[0]):\n",
    "    accession_id = metadata.accession_id[row]\n",
    "    seq_run = metadata.seq_run[row]\n",
    "    \n",
    "    fasta_file_path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/fasta_files/%s/%s_consensus_renamed.fa' % (seq_run, accession_id)\n",
    "    \n",
    "    print('adding %s_consensus_renamed.fa from %s to concatenated fasta file' % (accession_id, seq_run))\n",
    "    record = SeqIO.read(fasta_file_path, 'fasta')\n",
    "    with open(out_fasta, 'a') as out_handle:\n",
    "        SeqIO.write(record, out_handle, 'fasta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc652d",
   "metadata": {},
   "source": [
    "# filter samples to only omicron samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathway to original metadata internal file\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/metadata-internal.tsv'\n",
    "omicron = pd.read_csv(path,  sep=\"\\t\", dtype = {'accession_id' : object})\n",
    "print(\"original\", omicron.shape)\n",
    "\n",
    "# filtering date 6 months prior to current date, NEXTSTRAIN can't handle >50,000 samples in a build\n",
    "omicron['date2'] = pd.to_datetime(omicron['date'])  \n",
    "mask = (omicron['date2'] >= \"2022-01-01\")\n",
    "filtered = omicron.loc[mask]\n",
    "print(\"filtered date\", filtered.shape)\n",
    "\n",
    "# filtering down to B.1.1.529 south african strain\n",
    "a01 = filtered[\"pangolin\"].str.contains(\"BA\")==True\n",
    "\n",
    "df_filtered = filtered[ a01 ]\n",
    "print(\"filtered to omicron\", df_filtered)\n",
    "\n",
    "\n",
    "outfile = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/omicron_metadata-internal.tsv'\n",
    "df_filtered.to_csv(outfile, index = False, sep = '\\t')\n",
    "print (df_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eefd70",
   "metadata": {},
   "source": [
    "# filter cov samples to internal omicron variant only and pull fasta files in together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathway to omicron metadata file\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/omicron_metadata-internal.tsv'\n",
    "metadata = pd.read_csv(path, sep = '\\t', dtype = {'accession_id' : object})\n",
    "\n",
    "\n",
    "crit = metadata.percent_non_ambigous_bases >= 90\n",
    "metadata = metadata[crit]\n",
    "print(metadata.shape)\n",
    "metadata = metadata.reset_index(drop = True)\n",
    "\n",
    "\n",
    "# using the fitlered assembly metrics we want to pull out the fasta files for those sequences\n",
    "out_fasta = '/home/diana_ir/Documents/nextstrain/covid_sequencing/out_data/omicron_sequence_data.fasta'\n",
    "if os.path.isfile(out_fasta):\n",
    "    os.remove(out_fasta)\n",
    "\n",
    "for row in range(metadata.shape[0]):\n",
    "    accession_id = metadata.accession_id[row]\n",
    "    seq_run = metadata.seq_run[row]\n",
    "    \n",
    "    fasta_file_path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/fasta_files/%s/%s_consensus_renamed.fa' % (seq_run, accession_id)\n",
    "    \n",
    "    print('adding %s_consensus_renamed.fa from %s to concatenated fasta file' % (accession_id, seq_run))\n",
    "    record = SeqIO.read(fasta_file_path, 'fasta')\n",
    "    with open(out_fasta, 'a') as out_handle:\n",
    "        SeqIO.write(record, out_handle, 'fasta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578152b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c460cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
