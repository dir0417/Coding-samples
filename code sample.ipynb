{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing all the python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "\n",
    "#dates\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sb\n",
    "\n",
    "# biopython\n",
    "from Bio import SeqIO, AlignIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "# phylogenetics\n",
    "from ete3 import Tree, TreeStyle, NodeStyle, AttrFace, faces, CircleFace\n",
    "from ete3 import SeqMotifFace, add_face_to_node, TextFace, CircleFace\n",
    "from ete3 import Nexml\n",
    "\n",
    "# set display options\n",
    "pd.options.display.max_columns = 50\n",
    "os.environ['QT_QPA_PLATFORM']='offscreen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions\n",
    "bi_covid_dataset = 'path to file'\n",
    "concatenated_assembly_metrics = 'path to file'\n",
    "\n",
    "# wd = '~/buena_vista_study/'\n",
    "wd = 'path to file'\n",
    "os.chdir(wd)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the post_assembled_genomes.py script\n",
    "os.chdir('path to file')\n",
    "import indel_finder as indel_finder\n",
    "\n",
    "# import the nextclade parser function\n",
    "import nextclade_json_parser as parser\n",
    "\n",
    "# import clean horizon query script\n",
    "# import clean_horizon_cov_pui_query as clean_query\n",
    "import clean_horizon_covpui_varseq_fix_collection_date_2 as clean_query \n",
    "\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. merging pui and varseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = merging  bicovid and varseq\n",
    "bicovid_path = 'path to file'\n",
    "varseq_path = 'path to file'\n",
    "\n",
    "bicovid_varseq = clean_query.main(bicovid_path =  bicovid_path, varseq_path = varseq_path)\n",
    "bicovid_varseq = bicovid_varseq.reset_index(drop = True)\n",
    "bicovid_varseq.head()\n",
    "\n",
    "# save files\n",
    "outfile = os.path.join(wd, '01_bicovid_varseq.csv')\n",
    "bicovid_varseq.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. fix collection dates from pui and cv_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fix collection dates from  bi_covid and cv_var\n",
    "path = os.path.join(wd, '01_bicovid_varseq.csv')\n",
    "query = pd.read_csv(path, dtype = {\"accession_id\" : object})\n",
    "\n",
    "# drop test clients\n",
    "crit = query.cust_name != 'Test Client- For Testing only'\n",
    "query = query[crit]\n",
    "query = query.reset_index(drop = True)\n",
    "\n",
    "# replace missing collection_dates with receive dates\n",
    "crit = query.collection_date.isna()\n",
    "print('number of NAs collection dates: %d' % query[crit].shape[0])\n",
    "query.collection_date = query.collection_date.fillna('')\n",
    "print(crit.shape)\n",
    "\n",
    "# how many collection dates need to be corrected\n",
    "crit = query.collection_date < '2020-03-01'\n",
    "print('will we replace %d collection dates (before 2020-03-01) with receive dates' % query[crit].shape[0])\n",
    "print('')\n",
    "print(crit.shape)\n",
    "\n",
    "\n",
    "\n",
    "collection_date_mod_list = []\n",
    "\n",
    "for row in range(query.shape[0]):\n",
    "    \n",
    "    collection_date = query.collection_date[row]\n",
    "    receive_date = query.receive_date[row]\n",
    "#     print(collection_date, receive_date, query.cust_name[row], query.first_name[row])\n",
    "    \n",
    "    if collection_date < '2020-03-01' or collection_date == '':\n",
    "        collection_date_mod_list.append(receive_date)\n",
    "    else:\n",
    "        collection_date_mod_list.append(collection_date)\n",
    "        \n",
    "query['collection_date_mod'] = collection_date_mod_list\n",
    "query.head()\n",
    "\n",
    "\n",
    "outfile = os.path.join(wd, '02_collection_date_mod.csv')\n",
    "query.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join (wd, \"02_collection_date_mod.csv\")\n",
    "df = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "def addprefix (accession_id):\n",
    "    return 'CO-CDPHE-%s' % accession_id\n",
    "\n",
    "fasta_header = df.apply(lambda x: addprefix (x.accession_id), axis =1)\n",
    "df.insert(value = fasta_header, column = 'fasta_header', loc =0)\n",
    "df.dtypes\n",
    "\n",
    "# make identifier\n",
    "# to get the number of individuals\n",
    "def make_identifier(last_name, first_name, dob):\n",
    "    return \"%s_%s_%s\" % (last_name, first_name, dob)\n",
    "\n",
    "df.first_name = df.first_name.str.capitalize()\n",
    "df.first_name = df.first_name.str.strip()\n",
    "df.last_name = df.last_name.str.capitalize()\n",
    "df.last_name = df.last_name.str.strip()\n",
    "df.dob= pd.to_datetime(df.dob).dt.date\n",
    "\n",
    "# df.last_name= df.last_name.str.capitalize()\n",
    "\n",
    "\n",
    "\n",
    "identifier = df.apply(lambda x:make_identifier(x.last_name, x.first_name, x.dob), axis = 1)\n",
    "df.insert(value = identifier, column = \"person_id\", loc = 0)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# save\n",
    "outfile = os.path.join(wd, '02_collection_date_mod.csv')\n",
    "df.to_csv(outfile, index = False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the \"blank\" staff members into the dataset\n",
    "path = os.path.join (wd, 'path to file')\n",
    "filename = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "\n",
    "filename.head()\n",
    "filename.collection_date = pd.to_datetime(filename.collection_date, format = '%m/%d/%Y')\n",
    "filename = filename.rename(columns = {'collection_date' : 'collection_date_mod'})\n",
    "filename = filename.accession_id.unique().tolist()\n",
    "\n",
    "print(len(df.accession_id.unique().tolist())) # number of indivduals taht have tested positive \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering dataset to filename\n",
    "path = os.path.join (wd, \"02_collection_date_mod.csv\")\n",
    "df = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "crit = df.cust_name == 'filter file'\n",
    "crit2 = df.accession_id.isin(filename)\n",
    "filename_filtered = df[crit | crit2]\n",
    "print(filename_filtered.shape)\n",
    "\n",
    "# save\n",
    "outfile = os.path.join(wd, '03_filename_filtered.csv')\n",
    "filename_filtered.to_csv(outfile, index = False)\n",
    "filename_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates in filename filtered samples\n",
    "path = os.path.join (wd, \"03_filename_filtered.csv\")\n",
    "df = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "filename_dropdups = df.drop_duplicates(subset = 'person_id', keep = 'first')\n",
    "print(filename_dropdups.shape)\n",
    "\n",
    "# save\n",
    "outfile = os.path.join(wd, '04_filename_dropped_duplicates.csv')\n",
    "filename_dropdups.to_csv(outfile, index = False)\n",
    "filename_dropdups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring that person_id is only mentioned once\n",
    "path = os.path.join (wd, \"04_filename_dropped_duplicates.csv\")\n",
    "filename_person_id = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "\n",
    "# making sure that person_id are only mentioned once\n",
    "filename_person_id = filename_person_id.reset_index(drop=True)\n",
    "for row in range (filename_person_id.shape[0]) : \n",
    "    person_id = filename_person_id.person_id[row]\n",
    "    if person_id in corrections_dictionary.keys() : \n",
    "        print(person_id)\n",
    "        filename_person_id.at[row, 'person_id'] = corrections_dictionary [person_id]\n",
    "print(filename_person_id.shape)\n",
    "\n",
    "filename_person_id = filename_person_id.drop_duplicates(subset = 'person_id', keep = 'first')\n",
    "print(filename_person_id.shape)\n",
    "        \n",
    "# save\n",
    "outfile = os.path.join(wd, '04_filename_person_id.csv')\n",
    "filename_person_id.to_csv(outfile, index = False)\n",
    "filename_person_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open up the file\n",
    "path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/concat_metrics/concatenated_assembly_metrics_2022-04-14.csv'\n",
    "seq_metrics = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "drop_col = ['fasta_header']\n",
    "\n",
    "seq_metrics = seq_metrics.drop(columns= drop_col)\n",
    "\n",
    "path = os.path.join (wd, '04_filename_person_id.csv')\n",
    "filename_person_id = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "# merging pui & varseq with seq data\n",
    "seq_metrics = seq_metrics.sort_values(by = 'percent_non_ambigous_bases', ascending = False)\n",
    "seq_metrics = seq_metrics.drop_duplicates(subset = 'accession_id', keep = 'first')\n",
    "seq_metrics = seq_metrics.set_index('accession_id')\n",
    "filename_person_id = filename_person_id.set_index('accession_id')\n",
    "\n",
    "# joining concatenated assembly metrics with filtered data\n",
    "j = filename_person_id.join(seq_metrics, how = 'left')\n",
    "j = j.reset_index() #don't put drop because we don't want to drop accession id\n",
    "print(j.shape)\n",
    "\n",
    "# save\n",
    "outfile = os.path.join(wd, '05_seq_metrics_filename_person_id.csv')\n",
    "j.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open up the file\n",
    "path = os.path.join (wd, '05_seq_metrics_filename_person_id.csv')\n",
    "seq_metrics_filename_person_id = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "# drop NA in percent non ambiguous bases, unique to individuals\n",
    "na = seq_metrics_filename_person_id.dropna(axis=0, subset = ['percent_non_ambigous_bases'])\n",
    "print(na.shape)\n",
    "\n",
    "# drop duplicates\n",
    "na = na.drop_duplicates(subset = 'person_id', keep = 'first')\n",
    "print(na.shape)\n",
    "na\n",
    "\n",
    "# save\n",
    "outfile = os.path.join(wd, '06_droppedna_percent_non_ambiguous_bases.csv')\n",
    "na.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in files\n",
    "path = os.path.join (wd, '06_droppedna_percent_non_ambiguous_bases.csv')\n",
    "na = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "# has >90% coverage\n",
    "na = na.sort_values(by = 'percent_non_ambigous_bases', ascending = False)\n",
    "print(na.shape)\n",
    "\n",
    "crit = na.percent_non_ambigous_bases >= 90\n",
    "cov = na[crit]\n",
    "print(cov.shape)\n",
    "\n",
    "crit2 = na.percent_non_ambigous_bases <=90\n",
    "crit3 = na.percent_non_ambigous_bases >1\n",
    "nocov = na[crit2 & crit3]\n",
    "print(nocov.shape)\n",
    "\n",
    "crit4 = na.percent_non_ambigous_bases == 0\n",
    "zero = na[crit4]\n",
    "print(zero.shape)\n",
    "\n",
    "# save\n",
    "outfile = os.path.join(wd, '07_high_coverage_samples.csv')\n",
    "cov.to_csv(outfile, index = False)\n",
    "\n",
    "outfile = os.path.join(wd, '08_low_coverage_samples.csv')\n",
    "nocov.to_csv(outfile, index = False)\n",
    "\n",
    "outfile = os.path.join(wd, '08_zero_coverage_samples.csv')\n",
    "zero.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls lineages >90% high coverage samples, remove duplications\n",
    "path = os.path.join (wd, '07_high_coverage_samples.csv')\n",
    "high = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "# drop duplicates\n",
    "high = high.sort_values(by = 'collection_date')\n",
    "high = high.drop_duplicates(subset = 'person_id', keep = 'first')\n",
    "print(high.shape)\n",
    "high\n",
    "\n",
    "# save\n",
    "outfile = os.path.join(wd, '09_high_cov_drop_dups.csv')\n",
    "high.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire leftover samples that didn't link to TriCounty Epi data\n",
    "path = os.path.join (wd, '08_low_coverage_samples.csv')\n",
    "low = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "# drop duplicates\n",
    "low = low.sort_values(by = 'collection_date')\n",
    "low = low.drop_duplicates(subset = 'person_id', keep = 'first')\n",
    "print(low.shape)\n",
    "low\n",
    "\n",
    "# save\n",
    "outfile = os.path.join(wd, '10_low_cov_drop_dups.csv')\n",
    "low.to_csv(outfile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# high coverage --> assembly metrics and fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data up\n",
    "path = os.path.join (wd, '09_high_cov_drop_dups.csv')\n",
    "high = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "# merging with lineage\n",
    "outfile = '09_highcov_assembly_metrics.csv'\n",
    "high.to_csv(outfile, index = False)\n",
    "high\n",
    "\n",
    "# using filtered assembly metrics we want to pull to out the fasta files for those sequences\n",
    "out_fasta = '09_highcov_sequences.fasta'\n",
    "if os.path.isfile(out_fasta):\n",
    "    os.remove(out_fasta)\n",
    "\n",
    "n=0\n",
    "for row in range(high.shape[0]):\n",
    "    n=n+1\n",
    "    accession_id = high.accession_id[row]\n",
    "    seq_run = high.seq_run[row]\n",
    "    \n",
    "    fasta_file_path = '/home/diana_ir/Documents/nextstrain/covid_sequencing/fasta_files/%s/%s_consensus_renamed.fa' % (seq_run, accession_id)\n",
    "    \n",
    "    print(n, 'adding %s_consensus_renamed.fa from %s to concatenated fasta file' % (accession_id, seq_run))\n",
    "    record = SeqIO.read(fasta_file_path, 'fasta')\n",
    "    with open(out_fasta, 'a') as out_handle:\n",
    "        SeqIO.write(record, out_handle, 'fasta')\n",
    "        \n",
    "# on terminal, run pangolin on these files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file in\n",
    "path = os.path.join (wd, '09_highcov_assembly_metrics.csv')\n",
    "high = pd.read_csv(path, dtype = {'accession_id' : object})\n",
    "print(high.shape)\n",
    "\n",
    "# join lineage report with assembly metrics\n",
    "path = os.path.join (wd, '09_highcov_lineage_report.csv')\n",
    "pangolin = pd.read_csv(path, dtype = {'taxon': object})\n",
    "print(pangolin.shape)\n",
    "\n",
    "drop_col = ['pangolin_version']\n",
    "high = high.drop(columns=drop_col)\n",
    "\n",
    "# merge file together\n",
    "highcov_metrics_lineage = high.join(pangolin, how = 'left')\n",
    "highcov_metrics_lineage = highcov_metrics_lineage.reset_index()\n",
    "print(highcov_metrics_lineage.shape)\n",
    "\n",
    "# save file\n",
    "outfile = '11_highcov_metrics_pangolin.csv'\n",
    "highcov_metrics_lineage.to_csv(outfile, index= False)\n",
    "highcov_metrics_lineage.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# low coverage --> assembly metrics and fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data up\n",
    "path = os.path.join (wd, '10_low_cov_drop_dups.csv')\n",
    "low = pd.read_csv (path, dtype = {'accession_id' : object})\n",
    "\n",
    "low['lineage'] = 'low quality sequences'\n",
    "\n",
    "# save file\n",
    "outfile = '12_lowcov_metrics_lqseq.csv'\n",
    "low.to_csv(outfile, index= False)\n",
    "low.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combining high and low coverage metrics pangolin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "path = os.path.join (wd, '12_lowcov_metrics_lqseq.csv')\n",
    "low = pd.read_csv(path, dtype = {'accession_id' : object})\n",
    "print(low.shape)\n",
    "\n",
    "path = os.path.join (wd, '11_highcov_metrics_pangolin.csv')\n",
    "high = pd.read_csv(path, dtype = {'accession_id' : object})\n",
    "print(high.shape)\n",
    "\n",
    "\n",
    "# concatenate files together\n",
    "comb = pd.concat([high, low])\n",
    "comb.reset_index()\n",
    "print(comb.shape)\n",
    "\n",
    "# save file\n",
    "outfile = '13_combined_metrics_pangolin.csv'\n",
    "comb.to_csv(outfile, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read in 'no sequences' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "path = os.path.join (wd, '05_seq_metrics_filename_person_id.csv')\n",
    "na = pd.read_csv(path, dtype = {'person_id' : object})\n",
    "na = na.sort_values (by = 'person_id')\n",
    "print(na.shape)\n",
    "\n",
    "# creating a crit that drops high / low coverage\n",
    "crit = na['percent_non_ambigous_bases'].isnull()\n",
    "noseq = na[crit]\n",
    "print(noseq.shape)\n",
    "\n",
    "# drop duplicates\n",
    "noseq = noseq.sort_values(by = 'collection_date')\n",
    "noseq = noseq.drop_duplicates(subset = 'person_id', keep = 'first')\n",
    "print(noseq.shape)\n",
    "\n",
    "# save file\n",
    "outfile = '14_no_sequences.csv'\n",
    "noseq.to_csv(outfile, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "path = os.path.join (wd, '14_no_sequences.csv')\n",
    "noseq = pd.read_csv(path, dtype = {'person_id' : object})\n",
    "noseq = noseq.sort_values (by = 'person_id')\n",
    "print(noseq.shape)\n",
    "\n",
    "noseq['lineage'] = 'not sequenced'\n",
    "\n",
    "# save file\n",
    "outfile = '15_no_sequences_labeled.csv'\n",
    "noseq.to_csv(outfile, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combining high/low seq with noseq datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "path = os.path.join (wd, '13_combined_metrics_pangolin.csv')\n",
    "comb = pd.read_csv(path, dtype = {'accession_id' : object})\n",
    "print(comb.shape)\n",
    "\n",
    "path = os.path.join (wd, '15_no_sequences_labeled.csv')\n",
    "noseq = pd.read_csv(path, dtype = {'accession_id' : object})\n",
    "print(noseq.shape)\n",
    "\n",
    "# concatenate files together\n",
    "comb_noseq = pd.concat([comb, noseq])\n",
    "comb_noseq.reset_index()\n",
    "print(comb_noseq.shape)\n",
    "\n",
    "# save file\n",
    "outfile = '16_final_version.csv'\n",
    "comb_noseq.to_csv(outfile, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "path = os.path.join (wd, '16_final_version.csv')\n",
    "final = pd.read_csv(path, sep = '\\t', dtype = {'accession_id' : object})\n",
    "print(final.shape)\n",
    "\n",
    "# aggreate by date\n",
    "final.collection_date_mod = pd.to_datetime(final['collection_date_mod']) - pd.to_timedelta(7, unit= 'd')\n",
    "final = final.sort_values(by = 'collection_date_mod')\n",
    "\n",
    "\n",
    "weekly_counts = final.groupby(['lineage', pd.Grouper(key ='collection_date_mod', freq = 'W-SUN')]).size().unstack('lineage')\n",
    "weekly_counts = weekly_counts.reset_index()\n",
    "weekly_counts = weekly_counts.sort_values(by = 'collection_date_mod')\n",
    "weekly_counts = weekly_counts.reset_index(drop = True)\n",
    "\n",
    "\n",
    "weekly_counts.collection_date_mod = weekly_counts.collection_date_mod.dt.strftime('%d-%b-%Y')\n",
    "\n",
    "weekly_counts = weekly_counts.fillna(0)\n",
    "\n",
    "total_list = []\n",
    "for row in range(weekly_counts.shape[0]):\n",
    "    week_total = 0\n",
    "    for column in weekly_counts.columns:\n",
    "        if column != 'collection_date_mod':\n",
    "            week_total = int(week_total + weekly_counts.at[row, column])\n",
    "    total_list.append(week_total)\n",
    "weekly_counts['total'] = total_list\n",
    "weekly_counts\n",
    "\n",
    "# save file\n",
    "outfile = 'TABLE_weekly_counts.csv'\n",
    "weekly_counts.to_csv(outfile, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "path = os.path.join (wd, 'TABLE_weekly_counts.csv')\n",
    "weekly_counts = pd.read_csv(path, dtype = {'accession_id' : object})\n",
    "print(weekly_counts.shape)\n",
    "\n",
    "weekly_counts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineages = ['AY.100', 'AY.103', 'AY.122', 'AY.25', 'AY.26',\n",
    "       'AY.3', 'AY.3.1', 'AY.37', 'AY.39', 'AY.4', 'AY.44', 'AY.54', 'B.1',\n",
    "       'B.1.1.222', 'B.1.1.519', 'B.1.1.7', 'B.1.2', 'B.1.229', 'B.1.234',\n",
    "       'B.1.240', 'B.1.243', 'B.1.396', 'B.1.400', 'B.1.403', 'B.1.429',\n",
    "       'B.1.526', 'B.1.570', 'B.1.595', 'B.1.617.2', 'low quality sequences',\n",
    "       'not sequenced']\n",
    "lineages.sort(reverse = True)\n",
    "lineages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineages = ['not sequenced',\n",
    " 'low quality sequences',\n",
    " 'B.1.617.2',\n",
    " 'B.1.595',\n",
    " 'B.1.570',\n",
    " 'B.1.526',\n",
    " 'B.1.429',\n",
    " 'B.1.403',\n",
    " 'B.1.400',\n",
    " 'B.1.396',\n",
    " 'B.1.243',\n",
    " 'B.1.240',\n",
    " 'B.1.234',\n",
    " 'B.1.229',\n",
    " 'B.1.2',\n",
    " 'B.1.1.7',\n",
    " 'B.1.1.519',\n",
    " 'B.1.1.222',\n",
    " 'B.1',\n",
    " 'AY.54',\n",
    " 'AY.44',\n",
    " 'AY.4',\n",
    " 'AY.39',\n",
    " 'AY.37',\n",
    " 'AY.3.1',\n",
    " 'AY.3',\n",
    " 'AY.26',\n",
    " 'AY.25',\n",
    " 'AY.122',\n",
    " 'AY.103',\n",
    " 'AY.100']\n",
    "\n",
    "colors = [ 'orange', #1\n",
    "          'green', #2\n",
    "          'gold', #3\n",
    "         'olivedrab', #4\n",
    "         'lightseagreen',#5\n",
    "         'deepskyblue', #6\n",
    "         'navy', #7\n",
    "         'blueviolet', #8\n",
    "         'plum', #9\n",
    "         'purple', #10\n",
    "         'deeppink', #11\n",
    "          'yellow', #12\n",
    "        'red', #13\n",
    "         'pink', #14\n",
    "         'blue', #15\n",
    "         'darkorange', #16\n",
    "         'lightblue',#17\n",
    "         'orange', #18\n",
    "          'green', #19\n",
    "          'gold', #20\n",
    "         'olivedrab', #21\n",
    "         'lightseagreen',#22\n",
    "         'deepskyblue', #23\n",
    "         'navy', #24\n",
    "         'blueviolet', #25\n",
    "         'plum', #26\n",
    "         'purple', #27\n",
    "         'deeppink', #28\n",
    "          'yellow', #29\n",
    "           'red', #30\n",
    "         'pink', #31\n",
    "         'blue', #32\n",
    "         'darkorange', #33\n",
    "         'lightblue',#34\n",
    "          'gold' #35\n",
    "         ] \n",
    "color_dict = dict(zip(lineages, colors))\n",
    "color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC = weekly_counts.collection_date_mod.tolist()\n",
    "fig, ax = plt.subplots(figsize=(90,30))\n",
    "X =range(weekly_counts.shape[0])\n",
    "\n",
    "\n",
    "for i in range(len(lineages)):\n",
    "    print(i, lineages[i])\n",
    "    if i == 0:\n",
    "        #create the bar\n",
    "        plt.bar(x = X, \n",
    "                height = weekly_counts[lineages[i]], \n",
    "                color = color_dict[lineages[i]], \n",
    "                edgecolor = 'white', \n",
    "                width = 0.8 )\n",
    "    \n",
    "    else:\n",
    "        bottom_values = 0\n",
    "        for k in range(i):\n",
    "            if k < i :\n",
    "                bottom_values = bottom_values + weekly_counts[lineages[k]]\n",
    "        #create the bar\n",
    "        plt.bar(x = X, \n",
    "                height = weekly_counts[lineages[i]], \n",
    "                bottom = bottom_values, \n",
    "                color = color_dict[lineages[i]], \n",
    "                edgecolor = 'white', \n",
    "                width = 0.8 )\n",
    "    \n",
    "    \n",
    "# plt.bar(x=X, height = weekly_counts.total, width = 0.8)\n",
    "plt.style.use('seaborn-white')\n",
    "plt.title('')\n",
    "# plt.xlabel('date', fontsize = 56)\n",
    "plt.ylabel('number of individuals tested positive', fontsize = 80)\n",
    "plt.ylim((0, 200))\n",
    "\n",
    "plt.yticks(fontsize = 70)\n",
    "plt.xticks(X, WC, rotation = 90, fontsize = 60)\n",
    "\n",
    "plt.legend(color_dict.keys(), fontsize = 70, title = 'lineage', ncol=8)\n",
    "# plt.legend(ncol=3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "path = os.path.join(wd, 'FIGURE_epi_curve_bv_with_lineages.jpeg')\n",
    "plt.savefig(path)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
